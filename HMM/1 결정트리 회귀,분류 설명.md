[결정트리 기반 모델]에서는 

	하나의 트리에서 **같은 깊이(레벨)**에 위치한 여러 개의 리프 노드가 존재하는 것은 매우 일반적인 현상
	중요한 건: 한 테스트 샘플은 그 중 단 하나에만 도달합니다.
	회귀든 분류든 예외없이 
		1. 각 내부 노드(비리프 노드)는 특정 피처의 조건으로 분기
		2. 나의 테스트 샘플은 이진 분기 구조를 따라 루트에서 리프까지 **단 하나의 경로(path)**만 따라 이동
		3. 이 경로를 따라 도달한 **마지막 노드(리프 노드)**에서 예측값을 산출
			분류	: 도달한 학습 샘플들의 클래스 비율(확률)
			회귀	: 도달한 학습 샘플들의 타깃 평균값
				📌 즉, 도달 방식은 똑같고, 리프 노드에서 꺼내는 값만 다릅니다.

	✅ 아주 중요한 포인트
		✔️ 샘플은 분할할수록 계속 줄어든다.
		✔️ 결정트리는 각 분기점마다 "해당 노드에 남아있는 샘플"만 보고, 
		✔️ 샘플들의 피쳐를 다 보면서 (단, 랜덤포레스트는 여기서 피쳐 일부만 본다.)
		✔️ 최적의 분기 기준을 찾는다.



[결정트리 회귀] 

# 결정트리 회귀절차
	1. 모든 샘플의 모든 피쳐에 대해 각 샘플의 동일 피쳐들을 오름차순 정렬한 후 
	인접한 피쳐들의 평균값을 분할 후보지점으로 일단 둔 후에 (총후보지점의 개수는 샘플개수 n-1 * 피쳐개수 k)
	2. 모든 분할 후보지점에 대해 왼쪽 노드와 오른쪽 노드로 샘플을 나누고, 
	3. 각 노드 샘플들의 타깃 평균값을 예측값으로 삼아서 각 샘플들의 타깃과 MSE를 계산하고 그 총합을 구한다.
	4. MSE 총합이 최소가 되는 지점을 찾아 해당 지점을 루트 노드로 하여 왼쪽 노드와 오른쪽 노드 구분을 확정한 후 
	1~3를 반복해서 트리를 성장시킨다.
# 다음과 같은 종료 조건 중 하나를 만족하면 더 이상 분할하지 않음:
	1. 노드 내 샘플 수가 매우 적음
	2. MSE 감소가 너무 작거나
	3. 최대 트리 깊이에 도달

# 이 방식은 "최적의 단일 분할"을 찾는 데는 이론적으로 최선
	왜냐하면:
	모든 가능한 axis-aligned (축에 평행한) 이진 분할 중
	오차(MSE)를 최소화하는 유일한 분할을 찾기 때문
#⚠️ 단점: 계산량 폭증
# 대안
	1. Randomized Split
	전체 n−1개 후보 중 일부 샘플링
	ExtraTreesRegressor가 이런 방식 사용

	2. Histogram-based split
	(예: LightGBM, HistGradientBoosting)
	연속값을 고정된 bin 개수로 나눠서 분할 후보 수를 줄임
	후보 수를 256개 이하로 제한 가능 


	최상위 루트노드 분할
	|샘플 | x1 | x2 | y  |
	| 1 	 | 1 	 | 3  	| 10 |
	| 2  	| 2 	 | 1  	| 20 |
	| 3 	 | 3 	 | 2  	| 30 |
	| 4 	 | 4  	| 4 	 | 40 |

	| 샘플 | x1 | x2 | y  |
	| 2  	| 2 	 | 1  	| 20 |
	| 3 	 | 3 	 | 2  	| 30 |
	| 1 	 | 1 	 | 3  	| 10 |
	| 4 	 | 4  	| 4 	 | 40 |

	최적 분할기준은 x1<=2.5
	이기준으로 왼쪽 오른쪽 노드를 나눴을 때

	왼쪽 노드
	| 샘플 | x1 | x2 | y  |
	| 1 	 | 1 	 | 3  	| 10 |
	| 2  	| 2 	 | 1  	| 20 |

	오른쪽 노드
	| 샘플 | x1 | x2 | y  |
	| 3 	 | 3 	 | 2  	| 30 |
	| 4 	 | 4  	| 4 	 | 40 |

# 왜 타깃(레이블)의 평균을 예측값으로 쓰는가?
	그 노드 안의 모든 샘플에 대해 하나의 대표값으로 예측해야 할 때
	오차제곱합RSS이 가장  작아지는 대표값은 평균이기 때문이다.

=================================================

[결정트리 분류]
	클래스 0: 샘플 1, 2
	클래스 1: 샘플 3, 4

	| 샘플 | x1 | x2 | y(이진 분류 레이블) |
	| 1 	 | 1 	 | 3  	  | 0   |
	| 2  	| 2  	| 1  	  | 0   |
	| 3  	| 3  	| 2 	  | 1   |
	| 4  	| 4  	| 4 	  | 1   |

# 결정트리 분류 절차
	1. 모든 샘플의 모든 피쳐에 대해 각 샘플의 동일 피쳐들을 오름차순 정렬한 후
	인접한 피쳐들의 평균값을 분할 후보지점으로 일단 둔 후에 (총후보지점의 개수는 샘플개수 n-1 * 피쳐개수 k)
	2. 모든 분할 후보지점에 대해 1-p^2-(1-p)2 으로 지니불순도를 계산하고
	3. 모든 분할 후보지점마다 샘플을 왼쪽노드 오른쪽 노드로 나눠서 
	(왼쪽노드의 개수/전체노드의 개수) * 왼쪽 노드의 지니불순도 + 
	(오른쪽 노드의 개수/전체노드의 개수) * 오른쪽 노드의 지니불순도 
	= 왼쪽 오른쪽 노드의 가중평균 지니불순도가 최소가 되는 지점을 찾고
	4. 해당 지점을 루트노드로 하여 왼쪽 노드와 오른쪽 노드 구분을 확정한후, 각 자식노드의 샘플들을 대상으로 1~3을 반복해서 트리를 성장시킨다
# 다음과 같은 종료 조건 중 하나를 만족하면 더 이상 분할하지 않음:
	1. 노드 내 샘플 수가 매우 적음
	2. 노드가 완전히 순수 (지니불순도 = 0)
	3. 최대 트리 깊이에 도달

# 지니불순도를 1-p^2-(1-p)2 로 계산하는 이유 
	이진 분류일 경우
	클래스 0의 비율이 p이면 클래스 1 비율은 1−p, 
	무작위로 샘플 두 개를 독립적으로 뽑는다고 할 때:
		두 샘플이 모두 클래스 0일 확률 = p^2
		두 샘플 모두 클래스 1일 확률  = (1-p)^2

	즉 같은 클래스를 두번 뽑을 확률을 (이진 분류일 경우엔 양성, 음성일 경우) 
	다 구해 더한 후 
	그걸 전체 1에서 빼면
##	두 샘플이 서로 다른 클래스일 확률(불순도)이 된다.

	클래스가 k개 있으면
	클래스 k의 비율이 p 라면 두 샘플이 모두 K일 경우는 p^2
	따라서 이걸 모든 k에 대해 다 구한후 더한다음 1에서 빼면 일반화된다.

	이진분류에서 지니불순도 최대값은 1/2
	다중클래스에서 지니불순도 최대값은 (모든 클래스 비율이 같을 때이므로 p = 1/k 가 되어)  1 - k * (1/k)^2 = 1-(1/k) 가 된다. 

# 지니불순도로 최적 분할지점을 찾는 방법 상세
	(왼쪽 노드의 샘플수 / 부모 노드의 샘플수 ) * 왼쪽 노드의 지니불순도 + (오른쪽 노드의 샘플수 / 부모 노드의 샘플수 ) * 오른쪽 노드의 지니불순도가 가장 작은 지점 
	= 왼쪽 노드와 오른쪽 노드의 가중평균 지니불순도가 가장 작은 지점 
	= 루트노드의 지니불순도에서 자식노드 가중평균 지니불순도를 뺀값이 가장 큰 지점 
	= 불순도 감소량이 가장 큰 지점 
	= 정보이득이 가장큰 지점 (정보이득은 엔트로피 기준 분할평가 지표, 정확히 말하면 지니 감소량이 가장 큰 지점)
	= 분할지점 




