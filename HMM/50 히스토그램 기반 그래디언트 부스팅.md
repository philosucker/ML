# 히스토그램 기반 그래디언트 부스팅 (Histogram-based Gradient Boosting)

## 기본 그래디언트 부스팅의 문제
# 기존 그래디언트 부스팅 → 각 피쳐의 모든 샘플값을 정렬, 모든 분할 후보를 일일이 다 계산. -> 시간 복잡도 매우 높음

# 히스토그램 기반의 핵심 아이디어
	✔️ 연속형 피쳐 값을 ‘구간(빈, bin)’으로 미리 버킷화(이산화)한다.
	✔️ 즉, 연속값을 256개 bin 같은 ‘히스토그램 구간’으로 압축해서 처리한다.

	연속형 피쳐 → 256개 구간으로 미리 discretization.
	각 구간별로 그래디언트 합, 샘플 수 미리 계산.
	각 구간 경계만 분할 후보로 고려.
	분할 후보 수가 압도적으로 줄어듦 → 계산 속도 대폭 향상.

#	✅ 왜 빠른가?
	✔️ 기존: 모든 샘플값을 다 보는 O(n) → 너무 느림.
	✔️ 히스토그램 기반: bin 개수만 보면 됨 → O(b), b는 bin 개수 (보통 256).
	✔️ 메모리 캐시 최적화도 잘 됨 → 속도 극적으로 빨라짐.

#	✅ 정확도는 괜찮나?
	✔️ 약간의 근사 오차는 발생하지만 → 대부분의 경우 성능 손실은 거의 없음.
	✔️ 실제로 LightGBM, 최신 scikit-learn, CatBoost 전부 이 방법 씀.


# ✅ XGBoost vs LightGBM: 핵심 차이
	| 항목       			| XGBoost           			 | LightGBM          |
	| 노드 분할 방식 | Level-wise (수평 분할) | Leaf-wise (수직 분할) |
	| 속도      			 | 빠름               			  | 더 빠름              |
	| 메모리 사용 	  | 큼                 				 | 더 적음              |
	| 과적합 위험 	  | 상대적으로 낮음           | 상대적으로 높음          |
	| 병렬 처리 		   | 강점                				 | 더 강력              |
	| 대용량 데이터  	| 적당히 잘 처리           | 압도적으로 잘 처리        |
	| 히스토그램 기반 | 지원 (옵션)           	 | 기본 적용             |
	| 희소 데이터   		| 기본 지원             	 | 기본 지원             |


# 🔍 1. XGBoost: Level-wise 분할
	✔️ 한 레벨의 모든 노드를 동시에 확장 (균형 트리)
	✔️ 안정적 → 과적합 방어에 강함
	✔️ 하지만 연산량이 많아서 느릴 수 있음 , 소규모 데이터 셋에서 유리

# 🔍 2. LightGBM: Leaf-wise 분할
	✔️ 가장 손실 감소가 큰 리프 노드만 계속 확장 (불균형 트리)
	✔️ XGBoost 보다 훨씬 빠르고 깊은 트리 생성 가능, XGBoost보다 대규모 데이터에서 압도적으로 빠름.
	✔️ 과적합 위험이 XGBoost 보다 더 커질 수 있음 → min_data_in_leaf, max_depth 같은 규제 필수

# CatBoost는 다음과 같은 상황에 특히 유리함: 	✅ (특히 소규모에서 강력)
1. 범주형 변수가 많을 때 → 전처리 필요없음 자동 처리 + 정보 손실 없음
2. 데이터 수가 적거나 불균형할 때 → 과적합 덜 함
3. 튜닝 없이 기본값으로도 쓸만한 모델이 필요할 때



	
