# 엔트로피(Entropy)는 클래스의 불확실성을 측정하는 지표
엔트로피 정의:
이 값은 "그 노드에 포함된 클래스 분포의 혼잡도"를 의미해.

# 데스모스 엔트로피, 지니불순도

# 정보량은 log1/p = -logp 로 계산한다
## 로지스틱 손실 함수와 비슷하다 >>> 크로스 엔트로피와 관련
# 어떤 사건이 발생했을 때 그 사건이 발생할 확률이 매우 낮으면 0.01, 정보량은 많다.
# 확률이 높으면 정보량은 적다. 

이진 분류라면, 어떤 데이터의
	클래스 1일 확률(클래스1의 비율): p
	클래스 0일 확률(클래스0의 비율): 1−p 일때

# 엔트로피는 클래스가 1일 때의 정보량과, 클래스가 0일 때의 정보량의 기대값(평균)
### = 두 사건(클래스=1, 클래스=0) 각각이 줄 수 있는 “정보량”을 그 사건의 발생 확률로 가중평균한 값, 즉 평균 정보량(기댓 정보량)
 정보량 -logp 
	
#	𝐻(𝑝) = p⋅(-logp) + (1-p)⋅(-log(1-p))   
	= −𝑝log⁡𝑝−(1−𝑝)log⁡(1−𝑝)
# 로지스틱 손실과 굉장 비슷하다.
-[ylogp + (1-y)log(1-p)] 


# H(p)는 위로 볼록한 대칭형 곡선 그래프.

	H(p)는 확률이 0.5 일 때 최대가 된다. -0.5log0.5 - 0.5log0.5 = -log0.5 = log 2 = 1
	p가 0 또는 1일 때 H(p)는 0이 된다. 

# 정보량의 확률분포의 직관적 의미
	즉 가장 균등한 분포는 불확실성 최대,
	완전히 편중된 분포는 불확실성 없음

# 지니불순도와 비교
## ✅ 차이점 1: 수학적 정의의 본질이 다름
	| 구분  	 | 지니불순도                             											  | 엔트로피                              
	| 출발점 | 확률적 분류 오류 확률                     									  | 정보이론의 평균 정보량                      
	| 의미  	 | **두 샘플을 무작위로 뽑았을 때 서로 다른 클래스일 확률**	  | **샘플 하나의 정답을 알기 위해 필요한 정보량의 기대값** 
	| 계산량 | 빠름 (곱/제곱)                       												  | 상대적으로 느림 (로그)           

##✅ 유사점2: 그래프 모양
	지니: 포물선, 빠르게 0으로 내려감
	엔트로피: 곡선, 완만하게 0으로 내려감
	→ p가 0.7일 때도 지니는 0.42, 엔트로피는 0.881처럼 다르다

## ✅ 유사점 3: 트리 분할 결과
	두 함수 모두 같은 방향(불순도 감소)을 기준으로 분할을 하지만
	실제 트리 분기점이 미세하게 다를 수 있음
	하지만 실전에서 정확도 차이는 거의 없다

# 트리는 분할후 하위 노드의 데이터들이 더 작고 더 균일한 값들로 묶이더록 하는 분할기준 찾는 메커니즘

	더 작다는건 지니불순도, 엔트로피가 더 작아지는 쪽(정보량이 엄청 많거나 거의 없는 쪽)으로 분할한다는 것
	더 균일하다는 건  타깃값들이 더 비슷한 샘플들끼리 모이거나 정보량이 비슷한 샘플끼리 모인다는 것 -> 평균 예측이 더 유효해진다.

	🔸 📘 결정트리 회귀
		목적: MSE(예측값의 제곱 오차)를 줄이는 방향으로 노드를 나눔
		결과: 각 자식 노드 안의 y값이 더 비슷한 값들로 모이게 됨 (분산 ↓)
		→ 즉, **“더 균일한 값으로 묶이도록 분할 기준을 찾는다”**는 말은 정확히 맞음.

	🔸 📘 결정트리 분류
		목적: 자식 노드의 **불순도(지니, 엔트로피 등)**가 줄어들게 분할
		결과: 각 노드의 클래스 구성이 한 쪽 클래스에 더 편중됨 (순수도 ↑불확실성  ↓) = 	같은 클래스를 갖는 샘플들이 더 많이 한쪽 노드에 모이게 된다
		→ 즉, **“더 균일한 클래스(=예측 쉬운 상황)로 묶이도록 한다”**는 의미에서 이것도 맞는 표현


