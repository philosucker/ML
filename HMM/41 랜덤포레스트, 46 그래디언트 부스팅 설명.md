# 부트스트랩은 복원추출

# "데이터의 불확실성"(표본의 변동성)을 측정하는 도구
	1 표본 통계량의 분포를 추정 (평균, 표준편차, 중앙값 등)
	2 신뢰구간을 계산
	3 모델의 불확실성, 예측 오차를 추정

	우리가 어떤 데이터셋을 가지고 평균을 구했다고 하자.
	👉 이게 "우리 표본의 평균"이지, 모집단의 평균은 아닐 수도 있어.

	만약 이 데이터를 여러 번 부트스트랩 샘플링 하면,
	👉 부트스트랩 표본마다 평균이 조금씩 다를 거야.

	이 평균값들의 분포를 보면,
	👉 "내가 구한 평균이 얼마나 흔들릴 수 있는 값인가?" 를 관찰할 수 있어.

	그래서 부트스트랩은
	👉 "내가 가진 표본의 신뢰도, 불확실성, 오차 범위"를 추정하기 위한 매우 강력한 도구야.

	단순 표본 추출	
		한 번의 추정	내가 지금 뽑은 샘플에서 얻은 결과가 전부라고 가정
	부트스트랩 샘플링	
		변동성 추정	내가 가진 데이터가 우연히 이렇게 뽑혔을 뿐, 다른 표본이었다면 결과가 달랐을 수도 있다는 가정을 반영


[랜덤포레스트는] 다양성'을 강제로 만들어서 과적합을 줄이는 모델.
	"다양성"을 강제해서 편향과 분산의 균형을 맞추려는 모델이야.
	복원추출 (부트스트랩 샘플링) → 데이터 다양성 확보
	
# 결정트리는 특정 데이터의 노이즈에 과하게 반응하기 쉬운데,
# 랜덤포레스트는 개별 트리들의 과적합을 평균 내서 제거하는 거야.

## 결정트리:
#### 편향: 매우 낮음 (데이터에 완벽히 맞춤)
#### 분산: 매우 높음 (데이터 샘플이 바뀌면 결과가 크게 흔들림)

## 랜덤포레스트 / 엑스트라 트리:
#### 편향: 조금 증가 (데이터를 왜곡했으니까)
#### 분산: 크게 감소 (다양한 관점을 평균해서 안정됨)


# 랜덤포레스트 데이터셋 구성 방법
1. 전체데이터셋에서 복원추출을 해서 만든 서브데이터셋을 만든다. 
2. 이 서브데이터셋의 크기는 일반적으로 전체 데이터셋의 크기와 같게 한다.
3. 복원추출이므로 중복데이터가 들어간다.
4. 이런 방식으로 만든 서브데이터셋은 학습시킬 트리 개수만큼 만든다. 일반적으로 100~1000개.

## 각 트리를 학습시킬 때 노드 분할시 모든 분할후보지점에 대해 계산하지 않고 
## 무작위로 선택된 일부 피쳐에 대해서만 모든 가능한 분할후보지점을 탐색해서 최적 분할을 선택한다.
	피쳐 무작위 선택은 최소 최대 값중 랜덤 선택 
	테스트 시에는 모든 트리에서 예측을 하고
	회귀면 예측값들의 평균, 분류면 다수결로 한다.
	
# 랜덤샘플, 랜덤특성 사용으로 결정트리보다 연산량이 적어 훈련속도가 빠르고 과적합이 방지된다. 

# 랜덤포레스트의 하드보팅
	랜덤 포레스트는 각 트리가 예측한 클래스 레이블을 집계해서
	하드보팅으로 최종 클래스를 반환한다. -> rf.predict(X_test)

# 랜덤포레스트의 소프트보팅
	각 트리에서 테스트 샘플이 도달한 하나의 리프노드의 클래스 비율을 뽑아서
	같은 클래스끼리 평균을 내고
	그 평균값이 가장 큰 클래스를 최종 예측 클래스로 선택한다
	rf.predict_proba(X_test)

# OOB 샘플
	복원추출로 n개를 n번 뽑아 서브데이터셋을 만드는 랜덤포레스트는 그 샘플링 특성상
	어떤 샘플이 한번도 뽑히지 않을 확률이 트리마다 고정된다.
	(1-(1/n))^n = 약 0.368  즉 36.8% 의 샘플이 트리마다 사용되지 않는다. 
	이를 Out of Bag OOB 샘플로 따로 빼서 테스트 셋으로 사용할 수 있다.

=================================================

# 엑스트라트리 
	1. 부트스트랩 샘플을 쓰지 않고 전체 데이터셋을 사용.(부트스트랩을 옵션으로 줄수도 있다)
	2. 피쳐 선택도 랜덤포래스트 처럼 랜덤 
	3. 대신 무작위 선택된 피쳐의 분기점도 최적 분할지점을 탐색하는게 아니라 무작위로 분할. (랜덤 포레스트에 비해 더 높은 편향 더 낮은 분산)
		결정 기준을 고의로 부정확하게 만드는 구조.
		랜덤포레스트보다 무작위성이 좀 더 크다. 
		그래서 랜덤포레스트보다 더많은 트리를 훈련하지만 훈련속도는 더 빠르다.

=================================================
=================================================
# 43 부스팅 

	여러개의 분류기를 순차적으로 학습
	더 정확히 말하면 약한 학습기 (깊이 3정도의 트리) 수백개를 사용한다.
	adaboost, gradient boost 모두 전체 데이터 셋을 사용한다.
-----------------------------------------------------------------------------


# adaboost는 앞의 분류기에서 잘못 분류된 샘플을 찾아서 다음 트리가 그 샘플에 더 집중하게 만든다.

-----------------------------------------------------------------------------

# 그래디언트 부스트는 일반적인 트리처럼
	지니 불순도나 MSE 기준으로 피쳐를 선택하고, 임계값을 찾는다. 
	결정트리나 랜덤포레스트와 다른 것은
	아주 얕은 깊이의 트리 수백개를 순차적으로 학습시키는 방법에 있다.

# 그래디언트 부스트 회귀 는 앞의 분류기의 오차를 예측하도록 학습한다.
1. 처음엔 결정 트리 하나를 학습하고
2. 이 첫번째 트리가 예측한 값과 실제 값의 오차를 구한다.
3. 두번째 트리는 이 오차를 타겟레이블로 하여 예측하는 트리를 학습한다.
3. 세번째 트리는 두번째 트리의 오차를 예측하게 학습한다.
4. 마지막에 모든 트리의 예측값을 합산해서 최종 예측값을 만든다. 이 최종예측값은 모델이 내길 기대했던 가장 바람직한 예측값이 된다.

# 어떻게 이 최종예측값이 내가 원하는 값이 될까?

	이전 모델이 못 맞춘 오차를 계속 줄이는 것이 핵심.
	오차를 줄이는 방향으로 계속 이동하는 거라서 ‘그래디언트’라는 이름이 붙음.



# 그래디언트 부스팅 회귀는 손실함수로 MSE를 사용
정확히는 1/2(y-y^)**2  를 사용. (왜냐면 미분했을 때 결과를 더 간단하게 만들기 위해서)
# 기울기는 트리 예측값에 대한 손실함수의 미분값이
# 이 기울기가 다음 트리 예측값의 새로운 타깃.
	L =  1/2(y-y^)**2 
	dL/dy^ = d[1/2(y-y^)**2 ] / dy^ 
	여기서 y-y^ 을 u라고 하면
	L = 1/2 * u**2
	따라서 dL/dy^을 체인룰로 다음과 같이 다시 쓸 수 있다.
	dL/du * du/dy^ = d(1/2 u**2)/du * -1 = 1/2 * 2u  * -1 = u * -1 = (y-y^) * -1
	
# 두번쨰 트리부터 마지막 트리까지 각 트리들은 앞선 트리의 잔차 y -y^ 를 레이블로 학습을 한다.
## 각 트리의 최종 예측값과 레이블(앞선 트리의 잔차)의 차이를 모두 더하는 (학습률을 곱해 가중합하면) 연산은
## 곧 최초 결정트리의 예측값에 이후 모든 트리의 잔차 y -y^ 를 더해주는 연산과 같고
# 이는 매 단계에서 -dL/dy^ 음의 기울기를 더해주는 것과 같다. 
## 만약  dL/dy^ 이 양수 였다면 
	 현재 예측값이 조금 커지면 손실도 조금 커지므로 예측값을 줄여야 한다는 게 된다.
	 그러면 자연스럽게 -dL/dy^ 를 더해주게 되고 기울기의 음수를 빼주게 되므로 결과적으로 예측값이 감소한다.
		 즉 기울기가 양수라는건 잔차(y -y^)가 음수라는게 되고 이는 예측값이 너무 크니 예측값을 줄여야 한다는 게 된다.
		 잔차가 음수이므로 이를 그대로 더하면 예측값이 감소하게 되는 결과와 같다.
		 
## 반대로 기울기 dL/dy^ 이 음수 였다면 
	 예측값이 조금 커지면 손실은 줄어드므로 예측값을 키워야 한다는 게 된다.
	 그러면 자연스럽게 -( -dL/dy^) 를 더해주게 되고 양의 기울기를 더해주게 되므로 결과적으로 예측값이 증가한다.
		 즉 현재 기울기가 음수라는 건 잔차가 양수라는게 되고 이는 현재 예측값이 다소 작으니 예측값을 키워야 한다는 게 된다.
		 잔차는 양수이므로 이를 그대로 더하면 예측값이 증가하게 되는 결과와 같다. 
		 
	즉 잔차(y -y^ )를 예측하게 만든다는 건 기울기의 음수(-dL/dy^ )를 예측하도록 만든다는 게 되고
	음의 기울기를 갱신하고자 하는 값에 더해주는 방식으로 업데이트가 일어나므로 경사 하강법이 된다.

# 손실을 줄이기 위한 방향은 항상 음의 기울기 방향

	 
# ✅ 핵심 차이: 경사하강법의 역할
	✔️ 선형 모델:
	경사하강법 → 계수 w를 직접 업데이트

	✔️ 그래디언트 부스팅:
	경사하강법 → "다음 트리가 어떤 잔차를 줄여야 하는지" 방향을 알려줌.

	다음 트리의 구조를 새로 학습함 (계수 X, 트리 O)
	✔️ 즉, 선형 모델은 ‘하나의 모델’을 계속 수정하는데,
	그래디언트 부스팅은 ‘새로운 모델’을 계속 추가하는 구조야.

	✔️ 그래디언트 부스팅은 모델을 수정하지 않아.
	✔️ 모델을 계속 추가할 뿐이야.

# 그래디언트 부스팅 분류에서는 
	1. 첫번째 트리는 학습대상이 아니다.
	2. 첫번째 트리는 로짓 값을 내놓는다. 
	3. 두 번째 트리(실질적으로 첫 번째 학습되는 트리)는
	이전 스텝에서의 로짓에 대해 로지스틱 손실을 미분한 값의 음수 (즉, 음의 그래디언트) 를 타깃으로 삼아 학습한다.
	로지스틱 손실
	-[ylogp + (1-y)log(1-p)
	4. 세 번째 트리는 두 번째 트리까지의 누적된 예측값  에 대해 다시 로지스틱 손실을 미분한 기울기를 구해서,
	그 음수값을 타깃으로 학습한다.


